{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Agent\n",
    "\n",
    "This notebook has two goals:\n",
    "1. train an agent to map states to action values\n",
    "2. watch a trained agent to navigate the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages and set paths\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from dqn_agent import Agent     # import agent.py\n",
    "import pandas as pd\n",
    "\n",
    "# declare directory to save weights of trained network\n",
    "WEIGHTS_PATH = 'outputs/'\n",
    "# declare directory and filename of trained network weights\n",
    "WEIGHTS_FILE = None          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate environment make sure you replace the filename and path with one that matches your folder structure.\n",
    "env = UnityEnvironment(file_name=\"python/Banana.exe\", no_graphics=True)\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]  # reset the environment\n",
    "state = env_info.vector_observations[0]  # get the current state\n",
    "state_size = len(state)\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load existing model, if you've got weights\n",
    "if WEIGHTS_FILE is not None:\n",
    "    qnetwork_weights = torch.load(WEIGHTS_FILE) \n",
    "else:\n",
    "    qnetwork_weights = None\n",
    "\n",
    "# Initialise agent. Set duel=True if you want dueling Q\n",
    "agent = Agent(state_size, action_size, seed=0, duel=True, qnetwork_weights=qnetwork_weights)  # initialise agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have pre loaded weights in place. Set eps_start=0\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1, eps_end=0.01, eps_decay=0.993):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []             # track episode scores\n",
    "    yellow_bananas = []     # track episode yellow bananas\n",
    "    blue_bananas = []       # track episode blue bananas\n",
    "    steps = []              # track episode steps\n",
    "    epsilons = []           # track episode epsilons\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start  # initialize epsilon\n",
    "\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "\n",
    "        env_info = env.reset(train_mode=True)[brain_name]  # reset the environment\n",
    "        state = env_info.vector_observations[0]  # get the current state\n",
    "        score = 0  # initialize the score\n",
    "        n_steps = 0  # initialize steps\n",
    "        n_yellow_bananas = 0\n",
    "        n_blue_bananas = 0\n",
    "\n",
    "        while True:\n",
    "            action = agent.act(state, eps)\n",
    "            env_info = env.step(action)[brain_name]  # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]  # get the next state\n",
    "            reward = env_info.rewards[0]  # get the reward\n",
    "            done = env_info.local_done[0]  # see if episode has finished\n",
    "            score += reward  # update the score\n",
    "            n_steps += 1\n",
    "            if reward == -1:\n",
    "                n_blue_bananas += 1\n",
    "            if reward == 1:\n",
    "                n_yellow_bananas += 1\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state  # roll over the state to next time step\n",
    "            if done:  # exit loop if episode finished\n",
    "                break\n",
    "        \n",
    "        # append performance metrics to lists\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        steps.append(n_steps)\n",
    "        yellow_bananas.append(n_yellow_bananas)\n",
    "        blue_bananas.append(n_blue_bananas)\n",
    "        epsilons.append(eps)\n",
    "\n",
    "        eps = max(eps_end, eps_decay * eps)  # decrease epsilon\n",
    "\n",
    "        # track training episodes and save weight file checkpoints\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}\\tEpsilon: {:.4f}'.format(i_episode, np.mean(scores_window), eps))\n",
    "            weights_file_name = WEIGHTS_PATH + 'checkpoint_episode_' + str(i_episode) + '.pth'\n",
    "            torch.save(agent.qnetwork_local.state_dict(), weights_file_name)\n",
    "        if np.mean(scores_window) >= 13.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            # save trained model weights with a timestamp\n",
    "            weights_file_name = WEIGHTS_PATH + 'checkpoint_solved' + str(int(round(time.time(), 0))) + '.pth'\n",
    "            torch.save(agent.qnetwork_local.state_dict(), weights_file_name)\n",
    "            break\n",
    "\n",
    "    return scores, steps, yellow_bananas, blue_bananas, epsilons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the agent\n",
    "start_time = time.time()\n",
    "scores, steps, yellow_bananas, blue_bananas, epsilons = dqn()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put observations in lists\n",
    "columns = ['scores', 'steps', 'yellow_bananas', 'blue_bananas', 'epsilons']\n",
    "data = [scores, steps, yellow_bananas, blue_bananas, epsilons]\n",
    "\n",
    "# convert to dataframe\n",
    "df = pd.DataFrame(dict(zip(columns, data)))\n",
    "\n",
    "# calculate moving average\n",
    "df['Yellow Bananas Moving Avg 10'] = df['yellow_bananas'].rolling(window=10).mean()\n",
    "df['Blue Bananas Moving Avg 10'] = df['blue_bananas'].rolling(window=10).mean()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot banana collection\n",
    "plt.plot( 'Yellow Bananas Moving Avg 10', data=df, marker='', color='olive', linewidth=2)\n",
    "plt.plot( 'Blue Bananas Moving Avg 10', data=df, marker='', color='blue', linewidth=2)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scores and epsilon\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# plot episode scores\n",
    "ax1.set_xlabel('Episodes')\n",
    "ax1.set_ylabel('Episode Scores', color='olive')\n",
    "ax1.plot(df['scores'], color='olive')\n",
    "ax1.tick_params(axis='y', labelcolor='olive')\n",
    "\n",
    "# instantiate a dual y-axis for epsilon. two plots share the same x-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Epsilon', color='black')  \n",
    "ax2.plot(df['epsilons'], color='black')\n",
    "ax2.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "# set layout and show\n",
    "fig.tight_layout() \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
